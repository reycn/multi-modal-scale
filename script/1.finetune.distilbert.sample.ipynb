{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from pandarallel import pandarallel\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=16)\n",
    "\n",
    "\n",
    "model_save_path = \"../process/bert-baseline/category-binary/\"\n",
    "\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.truncation_side = \"left\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "# 1. accu\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# 2. f1\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return evaluate.load(\"f1\").compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# [] TODO: AUROC\n",
    "# auroc\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return evaluate.load(\"auroc\").compute(predictions=predictions, references=labels)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    \"../dataset/process/hateful_memes_gpt_scale/df_train_vision_scale.csv\"\n",
    ").drop(columns=[\"scale_text\"])\n",
    "df_train[\"text_org\"] = df_train[\"text\"]\n",
    "\n",
    "df_train[\"text\"] = (\n",
    "    \"\"\"Your task is to analyze this given image and its caption to identify if there’s any forms of hateful content. Try to focus on the presence of any element that relates to any of the following:\\n1. Sexual aggression:\\na. Homophobia and Transphobia: This category encompasses hate speech targeting LGBTQ+ individuals, including slurs, derogatory comments, and rhetoric that seeks to undermine or dehumanize people based on their sexual orientation or gender identity.\\nb. Misogyny and Sexism: This category includes hate speech directed at women or based on gender. It covers derogatory language, stereotypes, and rhetoric that perpetuate gender inequality, objectification, and violence against women.\\n2. Hate based on ideology:\\na. Political Hate Speech: This category includes hate speech that is politically motivated, often targeting individuals or groups based on their political beliefs or affiliations. It may include inflammatory language, threats, and rhetoric designed to polarize or incite violence within political contexts.\\n3. Racism and xenophobia:\\na. COVID-19 and Xenophobia: This category includes hate speech that arose during the COVID-19 pandemic, often targeting specific ethnic groups or nationalities. It includes xenophobic language blaming certain groups for the spread of the virus, as well as fear-mongering and scapegoating related to the pandemic.\\nb. Racism Against Black People: This category focuses on hate speech directed at Black individuals or communities. It includes racial slurs, stereotypes, dehumanization, and other forms of derogatory language that perpetuate racial discrimination and inequality.\\nc. Racist Hate Against Other Ethnic Groups: This category includes hate speech directed at various ethnic groups other than Black individuals. It covers a range of racial slurs, xenophobic language, dehumanization, and derogatory remarks targeting specific ethnicities or nationalities.\\nd. White Supremacy: This category includes hate speech promoting white supremacist ideology, often intertwined with Christian extremist views. It includes rhetoric that advocates for racial superiority, anti-immigrant sentiments, and the dehumanization of non-white groups, sometimes using religious justifications for these beliefs.\\n4. Bigotry:\\na. Anti-Muslim and Islamophobic Hate: This category comprises hate speech aimed at Muslims and Islam. It includes language that promotes fear, hatred, dehumanization, or prejudice against Muslim individuals or communities, often using religious and cultural references to incite hostility.\\nb. Anti-Semitic Hate: This category focuses on hate speech directed at Jewish people and Judaism. It includes references to historical anti-Semitic tropes, conspiracy theories, and other forms of rhetoric that seek to dehumanize or discriminate against Jewish individuals and communities.\\n5. Miscellaneous Hate Speech: This category captures various forms of hate speech that do not fit neatly into the other categories. It includes a wide range of derogatory language and expressions that target individuals or groups based on different aspects of identity or personal characteristics. This category includes hate speech that targets individuals based on their physical or mental disabilities. It often includes derogatory language that mocks or devalues people with disabilities, promoting harmful stereotypes and exclusion. The caption is about `\"\"\"\n",
    "    + df_train[\"text\"]\n",
    "    + \"`. If it’s hateful, return the `TRUE` else `FALSE`.\"\n",
    ")\n",
    "# df_train[\"text\"] = df_train[\"text\"].apply(\n",
    "#     lambda x: x[:-512] if len(x) > 512 else x\n",
    "# )  # truncate\n",
    "df_train[\"img\"] = df_train[\"img\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(\n",
    "    lambda x: x[-512:] if len(x) > 512 else x\n",
    ")  # truncate\n",
    "# df_train = df_train.rename(columns={\"scale\": \"label\"})\n",
    "# df_train[\"label\"] = df_train[\"label\"].apply(lambda x: 1 if x >= 5 else 0)\n",
    "df_train[\"label\"] = df_train[\"label\"].astype(int)\n",
    "df_train = df_train[[\"text\", \"label\"]]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_json(\n",
    "    \"../dataset/raw/hateful_memes_expanded/test_seen.jsonl\",\n",
    "    lines=True,\n",
    ")\n",
    "df_test = pd.concat(\n",
    "    [\n",
    "        df_test,\n",
    "        pd.read_json(\n",
    "            \"../dataset/raw/hateful_memes_expanded/test_unseen.jsonl\",\n",
    "            lines=True,\n",
    "        ),\n",
    "    ]\n",
    ").drop(columns=[\"id\", \"img\"])[[\"text\", \"label\"]]\n",
    "df_test[\"text_org\"] = df_test[\"text\"]\n",
    "\n",
    "df_test[\"text\"] = (\n",
    "    \"\"\"Your task is to analyze this given image and its caption to identify if there’s any forms of hateful content. Try to focus on the presence of any element that relates to any of the following:\\n1. Sexual aggression:\\na. Homophobia and Transphobia: This category encompasses hate speech targeting LGBTQ+ individuals, including slurs, derogatory comments, and rhetoric that seeks to undermine or dehumanize people based on their sexual orientation or gender identity.\\nb. Misogyny and Sexism: This category includes hate speech directed at women or based on gender. It covers derogatory language, stereotypes, and rhetoric that perpetuate gender inequality, objectification, and violence against women.\\n2. Hate based on ideology:\\na. Political Hate Speech: This category includes hate speech that is politically motivated, often targeting individuals or groups based on their political beliefs or affiliations. It may include inflammatory language, threats, and rhetoric designed to polarize or incite violence within political contexts.\\n3. Racism and xenophobia:\\na. COVID-19 and Xenophobia: This category includes hate speech that arose during the COVID-19 pandemic, often targeting specific ethnic groups or nationalities. It includes xenophobic language blaming certain groups for the spread of the virus, as well as fear-mongering and scapegoating related to the pandemic.\\nb. Racism Against Black People: This category focuses on hate speech directed at Black individuals or communities. It includes racial slurs, stereotypes, dehumanization, and other forms of derogatory language that perpetuate racial discrimination and inequality.\\nc. Racist Hate Against Other Ethnic Groups: This category includes hate speech directed at various ethnic groups other than Black individuals. It covers a range of racial slurs, xenophobic language, dehumanization, and derogatory remarks targeting specific ethnicities or nationalities.\\nd. White Supremacy: This category includes hate speech promoting white supremacist ideology, often intertwined with Christian extremist views. It includes rhetoric that advocates for racial superiority, anti-immigrant sentiments, and the dehumanization of non-white groups, sometimes using religious justifications for these beliefs.\\n4. Bigotry:\\na. Anti-Muslim and Islamophobic Hate: This category comprises hate speech aimed at Muslims and Islam. It includes language that promotes fear, hatred, dehumanization, or prejudice against Muslim individuals or communities, often using religious and cultural references to incite hostility.\\nb. Anti-Semitic Hate: This category focuses on hate speech directed at Jewish people and Judaism. It includes references to historical anti-Semitic tropes, conspiracy theories, and other forms of rhetoric that seek to dehumanize or discriminate against Jewish individuals and communities.\\n5. Miscellaneous Hate Speech: This category captures various forms of hate speech that do not fit neatly into the other categories. It includes a wide range of derogatory language and expressions that target individuals or groups based on different aspects of identity or personal characteristics. This category includes hate speech that targets individuals based on their physical or mental disabilities. It often includes derogatory language that mocks or devalues people with disabilities, promoting harmful stereotypes and exclusion. The caption is about `\"\"\"\n",
    "    + df_train[\"text\"]\n",
    "    + \"`. If it’s hateful, return the `TRUE` else `FALSE`.\"\n",
    ")\n",
    "\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(\n",
    "    lambda x: x[-512:] if len(x) > 512 else x\n",
    ")  # truncate\n",
    "df_test[\"label\"] = df_test[\"label\"].astype(int)\n",
    "# df_test[\"img\"] = df_test[\"img\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_dev = train_test_split(df_train, test_size=0.3, random_state=42)\n",
    "# df_train = Dataset.from_pandas(df_train)\n",
    "# df_dev = Dataset.from_pandas(df_dev)\n",
    "# df_test = Dataset.from_pandas(df_test)\n",
    "# print(df_train.shape, df_dev.shape, df_test.shape)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    # id2label=id2label,\n",
    "    # label2id=label2id,\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_save_path,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# Convert pd.DataFrame to Hugging Face Dataset using `datasets`\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "dev_dataset = Dataset.from_pandas(df_dev)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "# Set the format of the dataset to be compatible with PyTorch/TensorFlow\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "dev_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # return accuracy.compute(predictions=predictions, references=labels)\n",
    "    accuracy = evaluate.load(\"accuracy\").compute(\n",
    "        predictions=predictions, references=labels\n",
    "    )[\"accuracy\"]\n",
    "    precision = evaluate.load(\"precision\").compute(\n",
    "        predictions=predictions, references=labels\n",
    "    )[\"precision\"]\n",
    "    recall = evaluate.load(\"recall\").compute(\n",
    "        predictions=predictions, references=labels\n",
    "    )[\"recall\"]\n",
    "    f1 = evaluate.load(\"f1\").compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    # auroc = evaluate.load(\"roc_auc\", \"multiclass\").compute(\n",
    "    #     prediction_scores=predictions, references=labels\n",
    "    # )[\"roc_auc\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        # \"auroc\": auroc,\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"preds\"] = preds\n",
    "df_test[\"preds_label\"] = df_test[\"preds\"].copy()\n",
    "df_test.to_csv(\"../result/bert.finetuning/category-binary/preds.csv\", index=False)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = evaluate.load(\"accuracy\").compute(\n",
    "    predictions=df_test[\"preds_label\"], references=df_test[\"label\"]\n",
    ")\n",
    "\n",
    "# Precision\n",
    "precision = evaluate.load(\"precision\").compute(\n",
    "    predictions=df_test[\"preds_label\"], references=df_test[\"label\"]\n",
    ")\n",
    "\n",
    "# Recall\n",
    "recall = evaluate.load(\"recall\").compute(\n",
    "    predictions=df_test[\"preds_label\"], references=df_test[\"label\"]\n",
    ")\n",
    "\n",
    "# F1\n",
    "f1 = evaluate.load(\"f1\").compute(\n",
    "    predictions=df_test[\"preds_label\"], references=df_test[\"label\"]\n",
    ")\n",
    "# AUROC\n",
    "auroc = evaluate.load(\"roc_auc\").compute(\n",
    "    prediction_scores=df_test[\"preds_label\"], references=df_test[\"label\"]\n",
    ")[\"roc_auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pp\n",
    "\n",
    "# RESULTS DICT\n",
    "results = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"auroc\": auroc,\n",
    "}\n",
    "pp(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
